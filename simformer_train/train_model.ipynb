{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SimFormer on Mock Galaxy Data\n",
    "\n",
    "This notebook trains a SimFormer model on mock galaxy photometric data with:\n",
    "- Multiple surveys (Gaia, 2MASS, WISE, PS1, DECam)\n",
    "- Error embeddings from measurement uncertainties\n",
    "- Observed/unobserved mask embeddings\n",
    "- Age-balanced subsampling for uniform logAge coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T15:05:22.975159Z",
     "start_time": "2026-02-16T15:05:22.947336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from prepare_data import load_and_filter, generate_synthetic_errors\n",
    "\n",
    "from columns import (\n",
    "    INTRINSIC_COLS, TRUE_MAG_COLS, OBS_COLS, OBS_ERR_COLS,\n",
    "    ALL_VALUE_COLS, NUM_NODES, N_INTRINSIC, N_TRUE_MAG,\n",
    ")\n",
    "from train_mock_galaxy import (\n",
    "    build_arrays, compute_age_bin_indices, create_model, \n",
    "    make_epoch_callback\n",
    ") \n",
    "from transformer import Simformer\n",
    "from simflower import FlowMatchingTrainer\n",
    "from utils import make_condition_mask_generator\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load & Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = '/n/home12/sratzenboeck/data_local/mock/train_data_prepared.parquet'\n",
    "\n",
    "if \"parquet\" in fname:\n",
    "    df = pd.read_parquet(fname)\n",
    "else:\n",
    "    df = load_and_filter(data_path=fname, max_rad=5, min_log_age=5)\n",
    "    df = generate_synthetic_errors(df, unobs_frac=0.2, seed=0)\n",
    "    print(f'After distance cut (rad < 5 kpc): {len(df):,} stars')\n",
    "    fname_out = '/n/home12/sratzenboeck/data_local/mock/train_data_prepared.parquet'\n",
    "    df.to_parquet(fname_out, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build data, errors, masks, means, and standard devs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "data, data_errors, data_observed_mask, means, stds, log_err_mean, log_err_std = build_arrays(df)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save normalization stats\nnorm_path = os.path.join('/n/home12/sratzenboeck/data_local/mock/', 'norm_stats.npz')\nnp.savez(norm_path, means=means, stds=stds, columns=ALL_VALUE_COLS,\n         log_err_mean=log_err_mean, log_err_std=log_err_std)\nprint(f'  Normalization stats saved to {norm_path}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "train_infos = dict(\n    do_compile = True,\n    batch_size = 2**12,\n    lr = 1e-3,\n    inner_loop_size = 500,\n    patience = 20,\n    val_split = 0.2,\n    amp = True,\n    wandb = True,\n    wandb_project = 'test_mock_galaxy',\n    n_bins = 30,\n    lr_min = 1e-5,\n    epochs = 500,\n    tau_max = 0.7, \n    tau_warmup = 100,\n    dense_ratio = 0.9,\n    cap_per_bin = 1000,\n)\ndo_compile = True"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ---- Age bin indices for curriculum weighting ----\n# We need bin indices that correspond to the training split.\n# The trainer does train_test_split internally, so we replicate the split\n# to get the correct bin indices for the training subset.\nlog_age = df['logAge'].values\nbin_idx_all, _ = compute_age_bin_indices(log_age, train_infos['n_bins'])\n\nn_total = len(data)\nall_indices = np.arange(n_total)\ntrain_indices, val_indices = train_test_split(\n    all_indices, test_size=train_infos['val_split'], random_state=42\n)\nbin_idx_train = bin_idx_all[train_indices]\nbin_idx_val = bin_idx_all[val_indices]\nbin_counts_train = np.bincount(bin_idx_train, minlength=train_infos['n_bins']).astype(np.float64)\nbin_counts_val = np.bincount(bin_idx_val, minlength=train_infos['n_bins']).astype(np.float64)\nprint(f\"  Curriculum: {train_infos['n_bins']} bins, train={len(train_indices):,}, val={len(val_indices):,}\")"
  },
  {
   "cell_type": "code",
   "source": "# --- Diagnostic: visualize cap-based sampling for various τ values ---\nfrom train_mock_galaxy import build_epoch_indices\n\ntau_values = [0.0, 0.2, 0.5, 0.7, 1.0]\ncap = train_infos['cap_per_bin']\nlog_age_train = log_age[train_indices]\n\n# Compute bin edges for consistent histogram bins\nbin_edges_hist = np.linspace(log_age.min(), log_age.max(), 50)\n\nfig, axes = plt.subplots(2, 3, figsize=(16, 9), sharex=True)\naxes = axes.ravel()\n\n# First panel: original (natural) distribution\naxes[0].hist(log_age_train, bins=bin_edges_hist, alpha=0.8, color='grey', density=True)\naxes[0].set_title(f'Original distribution\\n({len(log_age_train):,} stars)', fontsize=11)\naxes[0].set_ylabel('Density')\n\n# Remaining panels: cap-based sampling at each τ\nrng_diag = np.random.default_rng(0)\nfor i, tau in enumerate(tau_values):\n    ax = axes[i + 1]\n    idx = build_epoch_indices(bin_idx_train, bin_counts_train, tau, cap, rng_diag)\n    selected_ages = log_age_train[idx]\n\n    ax.hist(selected_ages, bins=bin_edges_hist, alpha=0.8, density=True,\n            color='steelblue' if tau < 0.99 else 'coral')\n    ax.set_title(f'τ = {tau:.1f}  ({len(idx):,} stars)', fontsize=11)\n\n    if i + 1 >= 3:\n        ax.set_xlabel('logAge')\n    if (i + 1) % 3 == 0:\n        ax.set_ylabel('Density')\n\nplt.suptitle(f'Cap-based epoch sampling (cap_per_bin={cap}, n_bins={train_infos[\"n_bins\"]})',\n             fontsize=13, y=1.01)\nplt.tight_layout()\nplt.show()\n\n# Print per-bin stats at τ=0\nprint(f\"\\n--- Per-bin counts at τ=0 (cap={cap}) ---\")\nprint(f\"{'Bin':>4}  {'Total':>8}  {'Selected':>8}  {'logAge range'}\")\nprint(\"-\" * 50)\nbin_edges = np.linspace(log_age.min(), log_age.max() + 1e-6, train_infos['n_bins'] + 1)\nrng_diag2 = np.random.default_rng(0)\nidx_tau0 = build_epoch_indices(bin_idx_train, bin_counts_train, 0.0, cap, rng_diag2)\nsel_bin_idx = bin_idx_train[idx_tau0]\nsel_counts = np.bincount(sel_bin_idx, minlength=train_infos['n_bins'])\nfor b in range(train_infos['n_bins']):\n    lo, hi = bin_edges[b], bin_edges[b + 1]\n    n_total = int(bin_counts_train[b])\n    n_sel = int(sel_counts[b])\n    flag = \" (all)\" if n_sel == n_total else \"\"\n    print(f\"{b:4d}  {n_total:8,}  {n_sel:8,}  [{lo:.2f}, {hi:.2f}){flag}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model ---\n",
      "  Model created: 816,225 parameters\n",
      "  Compiling model with torch.compile ...\n"
     ]
    }
   ],
   "source": [
    "# ---- Model ----\n",
    "print('\\n--- Model ---')\n",
    "model = create_model()\n",
    "if do_compile:\n",
    "    print('  Compiling model with torch.compile ...')\n",
    "    model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trainer ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msebastian-ratzenboeck\u001b[0m (\u001b[33msebastian-ratzenboeck-harvard-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.3s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/n/home12/sratzenboeck/code/simformer_train/wandb/run-20260216_181253-oprgx36g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sebastian-ratzenboeck-harvard-university/test_mock_galaxy/runs/oprgx36g' target=\"_blank\">sage-terrain-7</a></strong> to <a href='https://wandb.ai/sebastian-ratzenboeck-harvard-university/test_mock_galaxy' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sebastian-ratzenboeck-harvard-university/test_mock_galaxy' target=\"_blank\">https://wandb.ai/sebastian-ratzenboeck-harvard-university/test_mock_galaxy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sebastian-ratzenboeck-harvard-university/test_mock_galaxy/runs/oprgx36g' target=\"_blank\">https://wandb.ai/sebastian-ratzenboeck-harvard-university/test_mock_galaxy/runs/oprgx36g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trainer initialized.\n"
     ]
    }
   ],
   "source": [
    "# ---- Condition mask generator ----\n",
    "obs_indices = list(range(N_INTRINSIC + N_TRUE_MAG, NUM_NODES))\n",
    "cond_gen = make_condition_mask_generator(\n",
    "    batch_size=train_infos['batch_size'],\n",
    "    num_features=NUM_NODES,\n",
    "    percent=(0.1, 0.5),\n",
    "    allowed_idx=obs_indices,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# ---- Trainer ----\n",
    "print('\\n--- Trainer ---')\n",
    "trainer = FlowMatchingTrainer(\n",
    "    model=model,\n",
    "    data=data,\n",
    "    data_errors=data_errors,\n",
    "    data_observed_mask=data_observed_mask,\n",
    "    condition_mask_generator=cond_gen,\n",
    "    batch_size=train_infos['batch_size'],\n",
    "    lr=train_infos['lr'],\n",
    "    dense_ratio=train_infos['dense_ratio'],\n",
    "    inner_train_loop_size=train_infos['inner_loop_size'],\n",
    "    early_stopping_patience=train_infos['patience'],\n",
    "    val_split=train_infos['val_split'],\n",
    "    use_amp=train_infos['amp'],\n",
    "    use_wandb=train_infos['wandb'],\n",
    "    wandb_project=train_infos['wandb_project'],\n",
    "    wandb_config=train_infos,\n",
    "    device=device,\n",
    ")\n",
    "print('  Trainer initialized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready to train...\n"
     ]
    }
   ],
   "source": [
    "print('ready to train...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ---- LR scheduler ----\nlr_scheduler = CosineAnnealingLR(\n    trainer.optimizer, T_max=train_infos['epochs'], eta_min=train_infos['lr_min']\n)\n\n# ---- Epoch callback (curriculum) ----\nepoch_cb = make_epoch_callback(\n    bin_idx_train=bin_idx_train,\n    bin_counts_train=bin_counts_train,\n    bin_idx_val=bin_idx_val,\n    bin_counts_val=bin_counts_val,\n    tau_max=train_infos['tau_max'],\n    tau_warmup=train_infos['tau_warmup'],\n    cap_per_bin=train_infos['cap_per_bin'],\n    use_wandb=train_infos['wandb'],\n)\n\n# ---- Train ----\nprint(f\"\\n--- Training ({train_infos['epochs']} epochs) ---\")\nt0 = time.time()\nbest_model = trainer.fit(\n    epochs=train_infos['epochs'],\n    verbose=True,\n    epoch_callback=epoch_cb,\n    lr_scheduler=lr_scheduler,\n)\nelapsed = time.time() - t0\nprint(f'\\nTraining completed in {elapsed / 60:.1f} minutes.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ---- Save best model ----\noutput_dir = '/n/home12/sratzenboeck/data_local/mock/'\nckpt_path = os.path.join(output_dir, 'best_model.pt')\ntorch.save(best_model.state_dict(), ckpt_path)\nprint(f'  Best model saved to {ckpt_path}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Quick Validation: Sample Posterior\n",
    "\n",
    "Condition on observed Gaia + 2MASS photometry and sample intrinsic parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sampling import sample_batched_flow, build_inference_edge_mask\n\n# Pick a test star (from validation set)\ntest_idx = 0\nx_test = torch.tensor(data[test_idx], dtype=torch.float32)\nx_test_errors = torch.tensor(data_errors[test_idx], dtype=torch.float32)\nx_test_observed = torch.tensor(data_observed_mask[test_idx], dtype=torch.float32)\n\n# Condition on observed photometry columns\ncond_col_names = [\n    'GAIA_GAIA3.G_mag_obs', 'GAIA_GAIA3.Gbp_mag_obs', 'GAIA_GAIA3.Grp_mag_obs',\n    '2MASS_H_mag_obs', '2MASS_J_mag_obs', '2MASS_Ks_mag_obs',\n    'parallax_obs',\n]\ncond_indices = [ALL_VALUE_COLS.index(c) for c in cond_col_names if c in ALL_VALUE_COLS]\n\n# Safety check: only condition on columns that are actually observed for this star\nobs_flags = x_test_observed[cond_indices]\nunobs_cond = [ALL_VALUE_COLS[cond_indices[i]] for i, o in enumerate(obs_flags) if o == 0]\nif unobs_cond:\n    print(f\"WARNING: dropping unobserved conditioned columns: {unobs_cond}\")\n    cond_indices = [idx for idx, o in zip(cond_indices, obs_flags) if o == 1]\n\nprint(f\"Conditioning on {len(cond_indices)} columns: {[ALL_VALUE_COLS[i] for i in cond_indices]}\")\n\nn_samples = 512\ncondition_mask = torch.zeros(n_samples, NUM_NODES, dtype=torch.float32)\ncondition_mask[:, cond_indices] = 1.0\n\ncondition_values = x_test.unsqueeze(0).repeat(n_samples, 1)\nnode_ids = torch.arange(NUM_NODES).unsqueeze(0).repeat(n_samples, 1)\n\n# Errors: full vector from test star (NaN for unobserved bands → zero embedding\n# via ErrorEmbed's nan_to_num; real errors for observed bands → informative embedding).\n# This matches training where the model always sees the star's full error vector.\nsample_errors = x_test_errors.unsqueeze(0).repeat(n_samples, 1)\n\n# Observed mask: the star's actual observation pattern (which bands were measured).\n# The tokenizer uses this to embed physically meaningful context.\nsample_observed = x_test_observed.unsqueeze(0).repeat(n_samples, 1)\n\n# Build fully-connected edge mask (no observed filtering at inference)\nedge_mask = build_inference_edge_mask(n_samples, NUM_NODES, device=device)\n\nbest_model.eval()\nsamples = sample_batched_flow(\n    model_fn=best_model,\n    shape=(n_samples,),\n    condition_mask=condition_mask,\n    condition_values=condition_values,\n    node_ids=node_ids,\n    edge_masks=edge_mask,\n    errors=sample_errors,\n    observed_mask=sample_observed,\n    steps=50,\n    device=device,\n)\n\nsamples = samples.squeeze(-1).cpu().numpy()  # (n_samples, NUM_NODES)\n# Denormalize\nsamples_denorm = samples * stds + means\nprint(f'Samples shape: {samples_denorm.shape}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Corner plot of intrinsic parameters\nplot_cols = ['logAge', 'feh', 'logT', 'logg', 'logL', 'Av']\nplot_indices = [ALL_VALUE_COLS.index(c) for c in plot_cols]\n\n# True values for this star\ntrue_vals = (x_test.numpy() * stds + means)[plot_indices]\n\nfig, axes = plt.subplots(len(plot_cols), len(plot_cols), figsize=(14, 14))\nfor i, col_i in enumerate(plot_cols):\n    for j, col_j in enumerate(plot_cols):\n        ax = axes[i, j]\n        if j > i:\n            ax.axis('off')\n            continue\n        if i == j:\n            ax.hist(samples_denorm[:, plot_indices[i]], bins=30, alpha=0.7, density=True)\n            ax.axvline(true_vals[i], color='red', lw=2, label='True')\n            if i == 0:\n                ax.legend(fontsize=8)\n        else:\n            ax.scatter(samples_denorm[:, plot_indices[j]], samples_denorm[:, plot_indices[i]],\n                      s=1, alpha=0.3)\n            ax.scatter(true_vals[j], true_vals[i], color='red', s=50, zorder=10, marker='x')\n        if j == 0:\n            ax.set_ylabel(col_i, fontsize=10)\n        if i == len(plot_cols) - 1:\n            ax.set_xlabel(col_j, fontsize=10)\n\nplt.suptitle('Posterior Samples (conditioned on Gaia + 2MASS + parallax)', fontsize=14)\nplt.tight_layout()\nplt.show()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-py_gpu_cuda12.4]",
   "language": "python",
   "name": "conda-env-.conda-py_gpu_cuda12.4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}